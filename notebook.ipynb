{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Performance with Adversarial Samples\n",
    "\n",
    "In this notebook, we create some adversarial samples of a dataset using the Fast Gradient Sign Method (FSGM) and show the performance of several different classifier models when evaluated on these samples. In order to demonstrate the generalization of adversarial samples, we construct them using a completely separate dataset than what is used to train the classifier models. This is to follow the scenario where an adversarial will not have explicit access to a model's paramaters or the exact data it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Gradient Sign Method\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) is an algorithm to produce adversarial samples by adding small changes to pre-existing samples from a classification dataset. These small changes are usually imperceptable to the human eye, but can cause the output of a deep learning model to change enough to alter class predictions. In order to compute which small changes will result in the greatest skew in model outputs, FGSM uses gradient information from the loss on a classifier model that was trained on the type of data to be altered. The FGSM therefore requires two things; a representative dataset that mirrors the data used for any models that will be attacked, and a model trained on this representative dataset. S \n",
    "\n",
    "When training a deep learning model, backpropagation relies on the fact that the gradient of the loss function w.r.t model parameters is the multi-dimensional direction in model paramater space corresponding to the greatest increase of loss. Backpropagation therefore updates model paramaters in the exact opposite direction of this gradient in order to decrease the loss each update step. For FGSM, we compute the gradient of the loss w.r.t the input sample instead, resulting in the multi-dimensional direction in the input space that corresponds to the greates increase of loss. Since the goal of FGSM is to construct a sample that *hurts* the performance of the model (i.e. *increasing* the loss), this gradient information is used directly to slightly alter the input sample, resulting in a large increase of loss after this error cascades through the model undergoing multiple layers of multiplicative computation.\n",
    "\n",
    "Given a trained classifier $f$, with input $x$, and loss function $L(f(x))$, the FGSM computes an adversarial sample $x_{adv}$ with the formula:\n",
    "\n",
    "\\begin{align}\n",
    "x_{adv} = x + \\epsilon * \\text{sgn}(\\nabla_x L(f(x)))\n",
    "\\tag{1}\n",
    "\\label{eq:fgsm}\n",
    "\\end{align}\n",
    "\n",
    "Where $\\epsilon$ is a small (strength indicator) number (i.e. 0.1) and the *sign* of the gradient is used instead of the actual gradient since this preserves enough information to be effective.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Since FGSM needs a dataset and a model trained on this dataset to compute adversarial samples, we create two independent paritions of our training data prior to training models for this project; one parition is used to provide the gradient information to FGSM when creating adversarial samples, and the other is used to train the remaining \"friendly\" classifier models, which are then evaluated on adversarially altered data from the test set. This setup ensures that our friendly models cannot pick up on any common dataset artifacts to defend against adversarial samples, since these samples are coinstructed using information from a completely disjoint dataset.\n",
    "\n",
    "In the following code block we set up our environment, initialize a test set dataloader, and load all models. You will have to alter the global variables `CONFIG`, `ADVERSARIAL_MODEL`, and `FRIENDLY_MODEL_DIR` to match locations and specifications for your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import glob\n",
    "\n",
    "from util.pytorch_utils import build_image_dataset\n",
    "from util.data_utils import generate_df_from_image_dataset\n",
    "from module.classifier import Classifier\n",
    "\n",
    "# local/model/data parameters\n",
    "config = {\n",
    "    'dataset_directory': '/home/dylan/datasets/cifar_png/',\n",
    "    'batch_size': 32,\n",
    "    'input_dimensions': (32, 32, 3),\n",
    "    'number_workers': 1,\n",
    "}\n",
    "\n",
    "# local file locations\n",
    "adversary_model_file = '/home/dylan/trained_model_files/pytorch/adversarial_project/adversarial_models/classifier.pt'\n",
    "fiendly_model_directory = '/home/dylan/trained_model_files/pytorch/adversarial_project/friendly_models/'\n",
    "\n",
    "# get local device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# generate filenames/labels df from image data directory\n",
    "data_dict = generate_df_from_image_dataset(config['dataset_directory'])\n",
    "\n",
    "# add number of classes in labels to config\n",
    "config['output_dimension'] = data_dict['train']['Label'].nunique()\n",
    "\n",
    "# add number of samples to config\n",
    "config['number_train'] = len(data_dict['train'])\n",
    "config['number_test'] = len(data_dict['test'])\n",
    "\n",
    "# build testing dataloader\n",
    "test_set, test_loader = build_image_dataset(\n",
    "    data_dict['test'],\n",
    "    image_size=config['input_dimensions'][:-1],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['number_workers']\n",
    ")\n",
    "\n",
    "# initialize and load adversarial model\n",
    "adv_classifier = Classifier(config['input_dimensions'], config['output_dimension'])\n",
    "adv_classifier.load_state_dict(torch.load(adversary_model_file, map_location=device))\n",
    "\n",
    "# get all friendly model files\n",
    "friendly_model_files = glob.glob(fiendly_model_directory+'*.pt')\n",
    "\n",
    "# initialize all models found in friendly model directory\n",
    "classifiers = [{\n",
    "    'model_file': model_file,\n",
    "    'name': model_file.split('/')[-1].strip('.pt'), \n",
    "    'model': Classifier(config['input_dimensions'], config['output_dimension'])\n",
    "} for model_file in friendly_model_files]\n",
    "\n",
    "# load all models found in friendly model directory\n",
    "for classifier in classifiers:\n",
    "    classifier['model'].load_state_dict(torch.load(classifier['model_file'], map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
